{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Loaded Successfully on: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch # type: ignore\n",
    "import librosa # type: ignore\n",
    "\n",
    "# Load processor & trained model\n",
    "model_path = \"D:/Speech_recognition/Model_Training/wav2vec2_finetuned\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"‚úÖ Model Loaded Successfully on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Load audio file\n",
    "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Convert audio to tensor\n",
    "    input_values = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True).input_values\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # Decode prediction\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toggle to Speak Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import wave\n",
    "import keyboard # type: ignore\n",
    "import os\n",
    "\n",
    "recording = False\n",
    "audio_data = []\n",
    "samplerate = 16000\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    \"\"\"Callback function to store recorded data in real-time\"\"\"\n",
    "    global audio_data, recording\n",
    "    if recording:\n",
    "        audio_data.append(indata.copy())\n",
    "\n",
    "def record_audio_toggle(output_filename=\"recorded_audio/recorded.wav\"):\n",
    "    global recording, audio_data\n",
    "    audio_data = []\n",
    "    \n",
    "    # Ensure the \"recorded_audio\" folder exists\n",
    "    os.makedirs(\"recorded_audio\", exist_ok=True)\n",
    "\n",
    "    print(\"Press 'R' to start/stop recording...\")\n",
    "\n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype=np.int16, callback=callback):\n",
    "        while True:\n",
    "            if keyboard.is_pressed(\"r\"):\n",
    "                recording = not recording  # Toggle recording state\n",
    "                if recording:\n",
    "                    print(\"Recording started...\")\n",
    "                else:\n",
    "                    print(\"Recording stopped.\")\n",
    "                    break\n",
    "                while keyboard.is_pressed(\"r\"):  # Prevent multiple toggles on a single press\n",
    "                    pass  \n",
    "\n",
    "    # Prevent saving an empty file\n",
    "    if not audio_data:\n",
    "        print(\"No audio recorded. File not saved.\")\n",
    "        return None\n",
    "\n",
    "    # Save as WAV file\n",
    "    audio_np = np.concatenate(audio_data, axis=0)\n",
    "    with wave.open(output_filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)  # 16-bit audio\n",
    "        wf.setframerate(samplerate)\n",
    "        wf.writeframes(audio_np.tobytes())\n",
    "\n",
    "    print(f\"Audio saved as {output_filename}\")\n",
    "    return output_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'R' to start/stop recording...\n",
      "Recording started...\n",
      "Recording stopped.\n",
      "Audio saved as recorded_audio/recorded.wav\n",
      "üìù Transcription: THIS PALE SMELL OF OLD BEER LINGERSIT TAKES HEAD TO BRING OUT THE ODOURA COLD DIPRESTORES HEALTH AND ZUSTA SALT PICKLE TASTES VINE WITH HAINCUCKLES ALL PASTORE ARE MY FAVOURITEA ZSTFUL FOOD IS THE HOT CROSS BUN\n"
     ]
    }
   ],
   "source": [
    "recorded_file = record_audio_toggle()\n",
    "transcription=transcribe_audio(recorded_file)\n",
    "print(\"üìù Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload File Feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING NLP Technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API Key\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# ‚úÖ Initialize Groq LLM\n",
    "groq_llm = ChatGroq(\n",
    "    model_name=\"mixtral-8x7b-32768\",  # Choose supported model\n",
    "    groq_api_key=groq_api_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def batch_refine_transcriptions(asr_outputs, history_file=\"transcriptions_history.txt\"):\n",
    "    if not asr_outputs:\n",
    "        return []\n",
    "    \n",
    "    # Load previous transcriptions as context if file exists\n",
    "    previous_transcriptions = []\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            previous_transcriptions = file.readlines()\n",
    "    \n",
    "    # Format transcriptions\n",
    "    formatted_transcriptions = \"\\n\".join([f\"{i+1}. {text}\" for i, text in enumerate(asr_outputs)])\n",
    "\n",
    "    # Create the prompt with history for better corrections\n",
    "    prompt = f\"\"\"\n",
    "    You are an advanced ASR correction assistant that fixes phonetic and contextual mistakes in transcriptions.\n",
    "    - Correct phonetic errors \n",
    "    - Correct name recognition \n",
    "    - Ensure proper grammar while preserving original meaning.\n",
    "    - Refer to previous corrected transcriptions when necessary.\n",
    "\n",
    "    Previous transcriptions:\n",
    "    {\"\".join(previous_transcriptions)}\n",
    "\n",
    "    Here are multiple ASR outputs. Correct each one and return ONLY the corrected texts, numbered accordingly:\n",
    "    {formatted_transcriptions}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get response from Groq API\n",
    "    response = groq_llm.invoke(prompt)\n",
    "    if not response or not hasattr(response, \"content\"):\n",
    "        return [\"‚ùå Error: No response from Groq API\"]\n",
    "    \n",
    "    corrected_texts = response.content.strip().split(\"\\n\")\n",
    "    corrected_texts = [text.split(\". \", 1)[1] if \". \" in text else text for text in corrected_texts]\n",
    "\n",
    "    # Save new transcriptions\n",
    "    with open(history_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        for text in corrected_texts:\n",
    "            file.write(text + \"\\n\")\n",
    "\n",
    "    return corrected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Corrected 1: This pale smell of old beer lingers. It takes head to bring out the odor. Cold dip restores health and zest. A salt pickle tastes vinegar with huckleberries. All pastries are my favorite. The most fulfilling food is the hot cross bun.\n"
     ]
    }
   ],
   "source": [
    "corrected_texts = batch_refine_transcriptions(transcription)\n",
    "for i, corrected in enumerate(corrected_texts):\n",
    "    print(f\"‚úÖ Corrected {i+1}: {corrected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Audio recorded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "samplerate = 16000\n",
    "duration = 5  # seconds\n",
    "audio_data = []\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    audio_data.append(indata.copy())\n",
    "\n",
    "with sd.InputStream(samplerate=samplerate, channels=1, dtype=np.int16, callback=callback):\n",
    "    sd.sleep(int(duration * 1000))\n",
    "\n",
    "if not audio_data:\n",
    "    print(\"‚ö†Ô∏è No audio recorded!\")\n",
    "else:\n",
    "    print(\"‚úÖ Audio recorded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_env",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
