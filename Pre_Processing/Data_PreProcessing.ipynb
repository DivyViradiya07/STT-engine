{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AccentDB as it has Indian Dialect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import librosa.display # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = r\"D:\\Speech_recognition\\AudioFiles\\indian\"  \n",
    "\n",
    "def load_audio_files(path):\n",
    "    \"\"\"Get a list of all WAV files in the dataset directory.\"\"\"\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "# Get all audio file paths\n",
    "audio_files = load_audio_files(DATASET_PATH)\n",
    "print(f\"Found {len(audio_files)} audio files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram(audio_path, target_sr=16000):\n",
    "    \"\"\"Load an audio file, convert to spectrogram.\"\"\"\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=target_sr)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128)\n",
    "    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    return log_spectrogram\n",
    "\n",
    "sample_spectrogram = extract_spectrogram(audio_files[0])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(sample_spectrogram, x_axis=\"time\", y_axis=\"mel\", sr=16000)\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram of Sample Audio\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"D:/Speech_recognition/Processed_Spectrograms/american\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) \n",
    "\n",
    "for i, file in enumerate(audio_files):\n",
    "    spec = extract_spectrogram(file)\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"spec_{i}.npy\"), spec)\n",
    "    print(f\"Saved spectrogram {i+1}/{len(audio_files)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import torchaudio # type: ignore\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained Wav2Vec2 model and tokenizer\n",
    "model_name = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa # type: ignore\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    # Loading audio and convert to 16kHz (required by Wav2Vec2)\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    # Converting waveform to tensor\n",
    "    input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # Decode the output\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"D:/Speech_recognition/AudioFiles/indian/speaker_01/indian_s01_001.wav\"  # Change this path\n",
    "transcription = transcribe_audio(audio_file)\n",
    "print(f\"Transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "OUTPUT_TRANSCRIPTIONS = \"D:/Speech_recognition/transcriptions.json\"\n",
    "\n",
    "DATASET_PATH = \"D:/Speech_recognition/AudioFiles/indian\"\n",
    "audio_files = [os.path.join(root, file) for root, _, files in os.walk(DATASET_PATH) for file in files if file.endswith(\".wav\")]\n",
    "transcriptions = {}\n",
    "\n",
    "for i, file in enumerate(audio_files):\n",
    "    print(f\"Processing {i+1}/{len(audio_files)}: {file}\")\n",
    "    transcript = transcribe_audio(file)\n",
    "    transcriptions[file] = transcript\n",
    "\n",
    "#results saved to json\n",
    "with open(OUTPUT_TRANSCRIPTIONS, \"w\") as f:\n",
    "    json.dump(transcriptions, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Transcriptions saved to {OUTPUT_TRANSCRIPTIONS}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
