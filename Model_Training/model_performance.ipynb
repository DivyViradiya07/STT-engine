{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Load processor & trained model\n",
    "model_path = \"D:/Speech_recognition/Model_Training/wav2vec2_finetuned\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Load audio file\n",
    "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Convert audio to tensor\n",
    "    input_values = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True).input_values\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # Decode prediction\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GET THE TRUST FUN TO THE BANK EARLY', 'THE GRASS AND BUSHES WERE WHET WITH DEW', 'SMALL CHILDREN CAME TO SEE HIM', 'A PINK SHELL WAS FOUND ON THE SANDY BEACH', \"SHE SAW A CAT IN THE NEIGHBOUR'S HOUSE\"]\n",
      "✅ ASR Accuracy: 60.00%\n",
      "⏳ Average Inference Time: 3.4395 seconds per file\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# New test sentences for ASR evaluation (Ground Truth)\n",
    "actual_transcripts = [\n",
    "    \"GET THE TRUST FUND TO THE BANK EARLY\",\n",
    "    \"THE GRASS AND BUSHES WERE WET WITH DEW\",\n",
    "    \"SMALL CHILDREN CAME TO SEE HIM\",\n",
    "    \"A PINK SHELL WAS FOUND ON THE SANDY BEACH\",\n",
    "    \"SHE SAW A CAT IN THE NEIGHBOUR'S HOUSE\",\n",
    "]\n",
    "\n",
    "# List of test audio files (Replace with real file paths)\n",
    "audio_files = [\n",
    "    r\"D:\\Speech_recognition\\Data\\test_audio01.wav\",\n",
    "    r\"D:\\Speech_recognition\\Data\\test_audio02.wav\",\n",
    "    r\"D:\\Speech_recognition\\Data\\test_audio03.wav\",\n",
    "    r\"D:\\Speech_recognition\\Data\\test_audio04.wav\",\n",
    "    r\"D:\\Speech_recognition\\Data\\test_audio05.wav\"\n",
    "]\n",
    "\n",
    "# Function to transcribe multiple audio files using ASR model\n",
    "def generate_asr_predictions(audio_files):\n",
    "    predicted_texts = []\n",
    "    \n",
    "    for audio in audio_files:\n",
    "        transcript = transcribe_audio(audio)  # Your ASR function\n",
    "        predicted_texts.append(transcript)\n",
    "\n",
    "    return predicted_texts\n",
    "\n",
    "# Generate ASR outputs dynamically\n",
    "predicted_transcripts = generate_asr_predictions(audio_files)\n",
    "print(predicted_transcripts)\n",
    "\n",
    "# Function to calculate simple accuracy (exact matches)\n",
    "def calculate_accuracy(actual, predicted):\n",
    "    correct = sum(1 for a, p in zip(actual, predicted) if a.lower() == p.lower())\n",
    "    accuracy = (correct / len(actual)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Run accuracy test\n",
    "accuracy = calculate_accuracy(actual_transcripts, predicted_transcripts)\n",
    "print(f\"✅ ASR Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Measure inference speed\n",
    "def measure_inference_time(audio_files):\n",
    "    times = []\n",
    "    \n",
    "    for audio in audio_files:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        _ = transcribe_audio(audio)  # Run ASR\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    return avg_time\n",
    "\n",
    "# Run inference time test\n",
    "avg_inference_time = measure_inference_time(audio_files)\n",
    "print(f\"⏳ Average Inference Time: {avg_inference_time:.4f} seconds per file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API Key\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# ✅ Initialize Groq LLM\n",
    "groq_llm = ChatGroq(\n",
    "    model_name=\"mistral-saba-24b\",  # Choose supported model\n",
    "    groq_api_key=groq_api_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "def batch_refine_transcriptions(asr_outputs, history_file=\"transcriptions_history.txt\"):\n",
    "    if not asr_outputs:\n",
    "        return []\n",
    "    \n",
    "    # Load previous transcriptions as context if file exists\n",
    "    previous_transcriptions = []\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            previous_transcriptions = file.readlines()\n",
    "    \n",
    "    # Format transcriptions\n",
    "    formatted_transcriptions = \"\\n\".join([f\"{i+1}. {text}\" for i, text in enumerate(asr_outputs)])\n",
    "\n",
    "    # Create the prompt with history for better corrections\n",
    "    prompt = f\"\"\"\n",
    "    You are an advanced ASR correction assistant that fixes phonetic and contextual mistakes in transcriptions.\n",
    "    - Correct phonetic errors \n",
    "    - Correct name recognition \n",
    "    - Ensure proper grammar while preserving original meaning.\n",
    "    - Refer to previous corrected transcriptions when necessary.\n",
    "\n",
    "    Previous transcriptions:\n",
    "    {\"\".join(previous_transcriptions)}\n",
    "\n",
    "    Here are multiple ASR outputs. Correct each one and return ONLY the corrected texts, numbered accordingly:\n",
    "    {formatted_transcriptions}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get response from Groq API\n",
    "    response = groq_llm.invoke(prompt)\n",
    "    if not response or not hasattr(response, \"content\"):\n",
    "        return [\"❌ Error: No response from Groq API\"]\n",
    "    \n",
    "    corrected_texts = response.content.strip().split(\"\\n\")\n",
    "    corrected_texts = [text.split(\". \", 1)[1] if \". \" in text else text for text in corrected_texts]\n",
    "\n",
    "    # Save new transcriptions\n",
    "    with open(history_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        for text in corrected_texts:\n",
    "            file.write(text + \"\\n\")\n",
    "\n",
    "    return corrected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GET THE TRUST FUND TO THE BANK EARLY', 'THE GRASS AND BUSHES WERE WET WITH DEW', 'SMALL CHILDREN CAME TO SEE HIM', 'A PINK SHELL WAS FOUND ON THE SANDY BEACH', \"SHE SAW A CAT IN THE NEIGHBOR'S HOUSE\"]\n",
      "✅ ASR Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Refined_transcripts = batch_refine_transcriptions(predicted_transcripts)\n",
    "\n",
    "def calculate_accuracy(actual, predicted):\n",
    "    correct = sum(1 for a, p in zip(actual, predicted) if a.lower() == p.lower())\n",
    "    accuracy = (correct / len(actual)) * 100\n",
    "    return accuracy\n",
    "\n",
    "print(Refined_transcripts)\n",
    "\n",
    "# Run accuracy test\n",
    "accuracy = calculate_accuracy(actual_transcripts, Refined_transcripts)\n",
    "print(f\"✅ ASR Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WER = Word Error Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 WER Before LLM Correction: 5.00%\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "# Calculate WER for each transcript\n",
    "wer_scores = [wer(actual, pred) for actual, pred in zip(actual_transcripts, predicted_transcripts)]\n",
    "\n",
    "# Compute Average WER\n",
    "avg_wer = sum(wer_scores) / len(wer_scores)\n",
    "\n",
    "print(f\"\\n🔍 WER Before LLM Correction: {avg_wer:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 WER After LLM Correction: 2.50% (Lower is better!)\n"
     ]
    }
   ],
   "source": [
    "# Apply LLM Refinement\n",
    "refined_transcripts = batch_refine_transcriptions(predicted_transcripts)\n",
    "\n",
    "# Compute WER After LLM Correction\n",
    "wer_after = sum(wer(actual, refined) for actual, refined in zip(actual_transcripts, refined_transcripts)) / len(actual_transcripts)\n",
    "\n",
    "print(f\"\\n🚀 WER After LLM Correction: {wer_after:.2%} (Lower is better!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_env",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
