{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wav2Vec2 Model Loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Wav2Vec2 Model Loaded on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import json\n",
    "\n",
    "# Function to preprocess audio\n",
    "def preprocess_audio(audio_path):\n",
    "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    return inputs.input_values.squeeze(0)  # Remove batch dim\n",
    "\n",
    "# Load dataset\n",
    "with open(\"D:/Speech_recognition/transcriptions.json\", \"r\") as f:\n",
    "    transcriptions = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoader Ready with 371 batches\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, transcriptions):\n",
    "        self.audio_paths = list(transcriptions.keys())\n",
    "        self.transcripts = list(transcriptions.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_values = preprocess_audio(self.audio_paths[idx])\n",
    "\n",
    "        # text transcription into tokenized labels (numerical tensor)\n",
    "        labels = processor(text=[self.transcripts[idx]], return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        labels = labels.squeeze(0).to(torch.long)  \n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = ASRDataset(transcriptions)\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    input_values = [item[\"input_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch] \n",
    "\n",
    "    # Pad audio inputs with zeros\n",
    "    input_values = pad_sequence(input_values, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Pad labels with -100 (special value ignored by CTC loss)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(\"✅ DataLoader Ready with\", len(train_loader), \"batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1   Loss= 245.63363647460938\n",
      "Batch: 2   Loss= 219.0860595703125\n",
      "Batch: 3   Loss= 149.3104248046875\n",
      "Batch: 4   Loss= 273.1280517578125\n",
      "Batch: 5   Loss= 102.82756042480469\n",
      "Batch: 6   Loss= 181.08494567871094\n",
      "Batch: 7   Loss= 185.3128204345703\n",
      "Batch: 8   Loss= 208.5411376953125\n",
      "Batch: 9   Loss= 236.97364807128906\n",
      "Batch: 10   Loss= 206.27926635742188\n",
      "Batch: 11   Loss= 198.99078369140625\n",
      "Batch: 12   Loss= 224.82431030273438\n",
      "Batch: 13   Loss= 229.7213592529297\n",
      "Batch: 14   Loss= 162.8397979736328\n",
      "Batch: 15   Loss= 252.51834106445312\n",
      "Batch: 16   Loss= 281.41729736328125\n",
      "Batch: 17   Loss= 171.29432678222656\n",
      "Batch: 18   Loss= 303.90081787109375\n",
      "Batch: 19   Loss= 315.43109130859375\n",
      "Batch: 20   Loss= 206.69808959960938\n",
      "Batch: 21   Loss= 372.7890319824219\n",
      "Batch: 22   Loss= 195.49945068359375\n",
      "Batch: 23   Loss= 157.0379638671875\n",
      "Batch: 24   Loss= 140.6300048828125\n",
      "Batch: 25   Loss= 151.03475952148438\n",
      "Batch: 26   Loss= 230.36160278320312\n",
      "Batch: 27   Loss= 148.67669677734375\n",
      "Batch: 28   Loss= 129.37774658203125\n",
      "Batch: 29   Loss= 163.90394592285156\n",
      "Batch: 30   Loss= 153.23355102539062\n",
      "Batch: 31   Loss= 150.0901336669922\n",
      "Batch: 32   Loss= 200.90145874023438\n",
      "Batch: 33   Loss= 227.6456298828125\n",
      "Batch: 34   Loss= 203.28245544433594\n",
      "Batch: 35   Loss= 138.7598419189453\n",
      "Batch: 36   Loss= 123.6595230102539\n",
      "Batch: 37   Loss= 95.43894958496094\n",
      "Batch: 38   Loss= 238.4473876953125\n",
      "Batch: 39   Loss= 163.41616821289062\n",
      "Batch: 40   Loss= 132.42132568359375\n",
      "Batch: 41   Loss= 150.77273559570312\n",
      "Batch: 42   Loss= 109.80765533447266\n",
      "Batch: 43   Loss= 142.50732421875\n",
      "Batch: 44   Loss= 170.93463134765625\n",
      "Batch: 45   Loss= 150.75025939941406\n",
      "Batch: 46   Loss= 126.45486450195312\n",
      "Batch: 47   Loss= 129.68475341796875\n",
      "Batch: 48   Loss= 136.1368865966797\n",
      "Batch: 49   Loss= 94.21497344970703\n",
      "Batch: 50   Loss= 125.75144958496094\n",
      "Batch: 51   Loss= 88.22831726074219\n",
      "Batch: 52   Loss= 138.70068359375\n",
      "Batch: 53   Loss= 132.78802490234375\n",
      "Batch: 54   Loss= 103.8011474609375\n",
      "Batch: 55   Loss= 96.39057922363281\n",
      "Batch: 56   Loss= 85.16862487792969\n",
      "Batch: 57   Loss= 81.81156158447266\n",
      "Batch: 58   Loss= 131.23190307617188\n",
      "Batch: 59   Loss= 115.57586669921875\n",
      "Batch: 60   Loss= 118.03624725341797\n",
      "Batch: 61   Loss= 39.134178161621094\n",
      "Batch: 62   Loss= 110.0274658203125\n",
      "Batch: 63   Loss= 187.39443969726562\n",
      "Batch: 64   Loss= 182.5867462158203\n",
      "Batch: 65   Loss= 161.73040771484375\n",
      "Batch: 66   Loss= 247.8082275390625\n",
      "Batch: 67   Loss= 140.27633666992188\n",
      "Batch: 68   Loss= 156.1405029296875\n",
      "Batch: 69   Loss= 216.11329650878906\n",
      "Batch: 70   Loss= 114.7823715209961\n",
      "Batch: 71   Loss= 122.96881866455078\n",
      "Batch: 72   Loss= 55.2236213684082\n",
      "Batch: 73   Loss= 155.22665405273438\n",
      "Batch: 74   Loss= 117.35911560058594\n",
      "Batch: 75   Loss= 132.9525146484375\n",
      "Batch: 76   Loss= 170.3309326171875\n",
      "Batch: 77   Loss= 206.048583984375\n",
      "Batch: 78   Loss= 127.6724853515625\n",
      "Batch: 79   Loss= 154.4195556640625\n",
      "Batch: 80   Loss= 79.44381713867188\n",
      "Batch: 81   Loss= 127.6064682006836\n",
      "Batch: 82   Loss= 82.2291259765625\n",
      "Batch: 83   Loss= 91.13752746582031\n",
      "Batch: 84   Loss= 122.34053802490234\n",
      "Batch: 85   Loss= 151.59112548828125\n",
      "Batch: 86   Loss= 77.76716613769531\n",
      "Batch: 87   Loss= 123.09086608886719\n",
      "Batch: 88   Loss= 157.82156372070312\n",
      "Batch: 89   Loss= 122.51448059082031\n",
      "Batch: 90   Loss= 139.51690673828125\n",
      "Batch: 91   Loss= 103.2138671875\n",
      "Batch: 92   Loss= 183.06422424316406\n",
      "Batch: 93   Loss= 134.5498046875\n",
      "Batch: 94   Loss= 154.1132049560547\n",
      "Batch: 95   Loss= 98.1654052734375\n",
      "Batch: 96   Loss= 128.37982177734375\n",
      "Batch: 97   Loss= 123.95518493652344\n",
      "Batch: 98   Loss= 131.26719665527344\n",
      "Batch: 99   Loss= 113.55654907226562\n",
      "Batch: 100   Loss= 141.6569366455078\n",
      "Batch: 101   Loss= 122.66055297851562\n",
      "Batch: 102   Loss= 104.0056381225586\n",
      "Batch: 103   Loss= 91.54621887207031\n",
      "Batch: 104   Loss= 104.92572784423828\n",
      "Batch: 105   Loss= 61.79118728637695\n",
      "Batch: 106   Loss= 135.64962768554688\n",
      "Batch: 107   Loss= 75.88141632080078\n",
      "Batch: 108   Loss= 84.02537536621094\n",
      "Batch: 109   Loss= 77.42616271972656\n",
      "Batch: 110   Loss= 63.25355911254883\n",
      "Batch: 111   Loss= 137.54270935058594\n",
      "Batch: 112   Loss= 168.29794311523438\n",
      "Batch: 113   Loss= 161.89376831054688\n",
      "Batch: 114   Loss= 158.4482421875\n",
      "Batch: 115   Loss= 118.33122253417969\n",
      "Batch: 116   Loss= 132.06411743164062\n",
      "Batch: 117   Loss= 185.5045166015625\n",
      "Batch: 118   Loss= 119.60549926757812\n",
      "Batch: 119   Loss= 228.081298828125\n",
      "Batch: 120   Loss= 299.012451171875\n",
      "Batch: 121   Loss= 226.47491455078125\n",
      "Batch: 122   Loss= 171.05267333984375\n",
      "Batch: 123   Loss= 173.0833282470703\n",
      "Batch: 124   Loss= 142.45501708984375\n",
      "Batch: 125   Loss= 113.98300170898438\n",
      "Batch: 126   Loss= 147.1165771484375\n",
      "Batch: 127   Loss= 89.73464965820312\n",
      "Batch: 128   Loss= 118.33969116210938\n",
      "Batch: 129   Loss= 90.68368530273438\n",
      "Batch: 130   Loss= 100.18119049072266\n",
      "Batch: 131   Loss= 140.923583984375\n",
      "Batch: 132   Loss= 100.2127685546875\n",
      "Batch: 133   Loss= 149.65032958984375\n",
      "Batch: 134   Loss= 81.1683578491211\n",
      "Batch: 135   Loss= 99.21968078613281\n",
      "Batch: 136   Loss= 122.4443130493164\n",
      "Batch: 137   Loss= 117.10017395019531\n",
      "Batch: 138   Loss= 108.8188705444336\n",
      "Batch: 139   Loss= 126.84159088134766\n",
      "Batch: 140   Loss= 93.14201354980469\n",
      "Batch: 141   Loss= 112.22998046875\n",
      "Batch: 142   Loss= 92.86871337890625\n",
      "Batch: 143   Loss= 68.90589904785156\n",
      "Batch: 144   Loss= 95.74372863769531\n",
      "Batch: 145   Loss= 83.02198791503906\n",
      "Batch: 146   Loss= 254.746826171875\n",
      "Batch: 147   Loss= 70.7806396484375\n",
      "Batch: 148   Loss= 126.71533203125\n",
      "Batch: 149   Loss= 103.73834228515625\n",
      "Batch: 150   Loss= 105.610107421875\n",
      "Batch: 151   Loss= 122.41360473632812\n",
      "Batch: 152   Loss= 82.92601776123047\n",
      "Batch: 153   Loss= 74.62448120117188\n",
      "Batch: 154   Loss= 124.23684692382812\n",
      "Batch: 155   Loss= 77.639404296875\n",
      "Batch: 156   Loss= 121.93896484375\n",
      "Batch: 157   Loss= 124.1407699584961\n",
      "Batch: 158   Loss= 103.30300903320312\n",
      "Batch: 159   Loss= 97.9999008178711\n",
      "Batch: 160   Loss= 57.62846374511719\n",
      "Batch: 161   Loss= 60.32456970214844\n",
      "Batch: 162   Loss= 103.78952026367188\n",
      "Batch: 163   Loss= 96.50279998779297\n",
      "Batch: 164   Loss= 49.49494934082031\n",
      "Batch: 165   Loss= 83.48125457763672\n",
      "Batch: 166   Loss= 116.49658203125\n",
      "Batch: 167   Loss= 93.5093994140625\n",
      "Batch: 168   Loss= 60.07512664794922\n",
      "Batch: 169   Loss= 115.42259216308594\n",
      "Batch: 170   Loss= 93.21714782714844\n",
      "Batch: 171   Loss= 74.0707778930664\n",
      "Batch: 172   Loss= 76.39640808105469\n",
      "Batch: 173   Loss= 141.16799926757812\n",
      "Batch: 174   Loss= 96.07720947265625\n",
      "Batch: 175   Loss= 116.10023498535156\n",
      "Batch: 176   Loss= 54.622100830078125\n",
      "Batch: 177   Loss= 51.211219787597656\n",
      "Batch: 178   Loss= 78.58781433105469\n",
      "Batch: 179   Loss= 111.00609588623047\n",
      "Batch: 180   Loss= 61.809871673583984\n",
      "Batch: 181   Loss= 141.12930297851562\n",
      "Batch: 182   Loss= 85.65813446044922\n",
      "Batch: 183   Loss= 70.99130249023438\n",
      "Batch: 184   Loss= 66.70555114746094\n",
      "Batch: 185   Loss= 68.99124145507812\n",
      "Batch: 186   Loss= 63.93170166015625\n",
      "Batch: 187   Loss= 87.98710632324219\n",
      "Batch: 188   Loss= 52.580604553222656\n",
      "Batch: 189   Loss= 69.34625244140625\n",
      "Batch: 190   Loss= 26.33623504638672\n",
      "Batch: 191   Loss= 113.26124572753906\n",
      "Batch: 192   Loss= 48.15296936035156\n",
      "Batch: 193   Loss= 40.774208068847656\n",
      "Batch: 194   Loss= 83.8690185546875\n",
      "Batch: 195   Loss= 69.64022064208984\n",
      "Batch: 196   Loss= 54.586727142333984\n",
      "Batch: 197   Loss= 58.96516799926758\n",
      "Batch: 198   Loss= 141.18508911132812\n",
      "Batch: 199   Loss= 60.912052154541016\n",
      "Batch: 200   Loss= 100.21123504638672\n",
      "✅ Epoch 1 Completed (Trained 200 batches)\n",
      "Batch: 1   Loss= 63.83285140991211\n",
      "Batch: 2   Loss= 86.60478973388672\n",
      "Batch: 3   Loss= 83.13151550292969\n",
      "Batch: 4   Loss= 70.12176513671875\n",
      "Batch: 5   Loss= 119.68887329101562\n",
      "Batch: 6   Loss= 49.652015686035156\n",
      "Batch: 7   Loss= 117.98209381103516\n",
      "Batch: 8   Loss= 127.73767852783203\n",
      "Batch: 9   Loss= 84.9827880859375\n",
      "Batch: 10   Loss= 95.51588439941406\n",
      "Batch: 11   Loss= 51.39971923828125\n",
      "Batch: 12   Loss= 172.82046508789062\n",
      "Batch: 13   Loss= 146.21514892578125\n",
      "Batch: 14   Loss= 107.06369018554688\n",
      "Batch: 15   Loss= 85.19927978515625\n",
      "Batch: 16   Loss= 102.25511169433594\n",
      "Batch: 17   Loss= 88.10509490966797\n",
      "Batch: 18   Loss= 86.68408203125\n",
      "Batch: 19   Loss= 80.11966705322266\n",
      "Batch: 20   Loss= 86.18852233886719\n",
      "Batch: 21   Loss= 92.072998046875\n",
      "Batch: 22   Loss= 127.31834411621094\n",
      "Batch: 23   Loss= 99.66799926757812\n",
      "Batch: 24   Loss= 115.57545471191406\n",
      "Batch: 25   Loss= 54.46638488769531\n",
      "Batch: 26   Loss= 111.60004425048828\n",
      "Batch: 27   Loss= 117.91725158691406\n",
      "Batch: 28   Loss= 61.1859130859375\n",
      "Batch: 29   Loss= 83.71371459960938\n",
      "Batch: 30   Loss= 84.16401672363281\n",
      "Batch: 31   Loss= 113.72504425048828\n",
      "Batch: 32   Loss= 66.4004135131836\n",
      "Batch: 33   Loss= 46.67610549926758\n",
      "Batch: 34   Loss= 76.04342651367188\n",
      "Batch: 35   Loss= 45.95573806762695\n",
      "Batch: 36   Loss= 66.32759094238281\n",
      "Batch: 37   Loss= 106.91004180908203\n",
      "Batch: 38   Loss= 71.45103454589844\n",
      "Batch: 39   Loss= 79.43457794189453\n",
      "Batch: 40   Loss= 104.01223754882812\n",
      "Batch: 41   Loss= 45.688987731933594\n",
      "Batch: 42   Loss= 46.98188781738281\n",
      "Batch: 43   Loss= 90.22836303710938\n",
      "Batch: 44   Loss= 68.83025360107422\n",
      "Batch: 45   Loss= 76.77587890625\n",
      "Batch: 46   Loss= 84.23580932617188\n",
      "Batch: 47   Loss= 61.46167755126953\n",
      "Batch: 48   Loss= 96.67961120605469\n",
      "Batch: 49   Loss= 87.92173767089844\n",
      "Batch: 50   Loss= 61.150856018066406\n",
      "Batch: 51   Loss= 45.626766204833984\n",
      "Batch: 52   Loss= 98.50764465332031\n",
      "Batch: 53   Loss= 140.95404052734375\n",
      "Batch: 54   Loss= 91.22050476074219\n",
      "Batch: 55   Loss= 50.94068145751953\n",
      "Batch: 56   Loss= 61.68159484863281\n",
      "Batch: 57   Loss= 76.08721923828125\n",
      "Batch: 58   Loss= 48.542396545410156\n",
      "Batch: 59   Loss= 73.81615447998047\n",
      "Batch: 60   Loss= 95.27018737792969\n",
      "Batch: 61   Loss= 26.479969024658203\n",
      "Batch: 62   Loss= 98.58749389648438\n",
      "Batch: 63   Loss= 57.782630920410156\n",
      "Batch: 64   Loss= 67.10433959960938\n",
      "Batch: 65   Loss= 71.8228988647461\n",
      "Batch: 66   Loss= 72.91242980957031\n",
      "Batch: 67   Loss= 18.997055053710938\n",
      "Batch: 68   Loss= 49.386009216308594\n",
      "Batch: 69   Loss= 58.747474670410156\n",
      "Batch: 70   Loss= 79.78152465820312\n",
      "Batch: 71   Loss= 81.84473419189453\n",
      "Batch: 72   Loss= 166.94496154785156\n",
      "Batch: 73   Loss= 66.72994995117188\n",
      "Batch: 74   Loss= 59.140830993652344\n",
      "Batch: 75   Loss= 40.878746032714844\n",
      "Batch: 76   Loss= 116.33909606933594\n",
      "Batch: 77   Loss= 24.022438049316406\n",
      "Batch: 78   Loss= 98.80778503417969\n",
      "Batch: 79   Loss= 60.78923034667969\n",
      "Batch: 80   Loss= 60.3797607421875\n",
      "Batch: 81   Loss= 71.23773193359375\n",
      "Batch: 82   Loss= 92.9649429321289\n",
      "Batch: 83   Loss= 72.89617919921875\n",
      "Batch: 84   Loss= 55.02337646484375\n",
      "Batch: 85   Loss= 98.33723449707031\n",
      "Batch: 86   Loss= 59.25764465332031\n",
      "Batch: 87   Loss= 65.37278747558594\n",
      "Batch: 88   Loss= 61.06013870239258\n",
      "Batch: 89   Loss= 107.93714141845703\n",
      "Batch: 90   Loss= 30.182231903076172\n",
      "Batch: 91   Loss= 101.57803344726562\n",
      "Batch: 92   Loss= 75.37556457519531\n",
      "Batch: 93   Loss= 46.242095947265625\n",
      "Batch: 94   Loss= 72.71510314941406\n",
      "Batch: 95   Loss= 74.56246948242188\n",
      "Batch: 96   Loss= 46.563446044921875\n",
      "Batch: 97   Loss= 31.383474349975586\n",
      "Batch: 98   Loss= 64.7185287475586\n",
      "Batch: 99   Loss= 125.9306640625\n",
      "Batch: 100   Loss= 117.41490173339844\n",
      "Batch: 101   Loss= 67.00665283203125\n",
      "Batch: 102   Loss= 57.561981201171875\n",
      "Batch: 103   Loss= 50.545318603515625\n",
      "Batch: 104   Loss= 62.003570556640625\n",
      "Batch: 105   Loss= 128.832275390625\n",
      "Batch: 106   Loss= 79.59364318847656\n",
      "Batch: 107   Loss= 84.57075500488281\n",
      "Batch: 108   Loss= 83.67201232910156\n",
      "Batch: 109   Loss= 87.01409149169922\n",
      "Batch: 110   Loss= 97.05684661865234\n",
      "Batch: 111   Loss= 87.74610900878906\n",
      "Batch: 112   Loss= 88.9374771118164\n",
      "Batch: 113   Loss= 118.72354125976562\n",
      "Batch: 114   Loss= 103.57319641113281\n",
      "Batch: 115   Loss= 59.440765380859375\n",
      "Batch: 116   Loss= 66.56076049804688\n",
      "Batch: 117   Loss= 55.59062957763672\n",
      "Batch: 118   Loss= 98.08432006835938\n",
      "Batch: 119   Loss= 70.44984436035156\n",
      "Batch: 120   Loss= 66.6441650390625\n",
      "Batch: 121   Loss= 89.89476013183594\n",
      "Batch: 122   Loss= 65.09573364257812\n",
      "Batch: 123   Loss= 68.8042221069336\n",
      "Batch: 124   Loss= 107.8445053100586\n",
      "Batch: 125   Loss= 73.86392211914062\n",
      "Batch: 126   Loss= 78.39371490478516\n",
      "Batch: 127   Loss= 55.542442321777344\n",
      "Batch: 128   Loss= 30.738967895507812\n",
      "Batch: 129   Loss= 117.0770263671875\n",
      "Batch: 130   Loss= 60.48616027832031\n",
      "Batch: 131   Loss= 62.09320831298828\n",
      "Batch: 132   Loss= 43.64086151123047\n",
      "Batch: 133   Loss= 43.50613021850586\n",
      "Batch: 134   Loss= 91.717041015625\n",
      "Batch: 135   Loss= 48.652503967285156\n",
      "Batch: 136   Loss= 97.96196746826172\n",
      "Batch: 137   Loss= 84.869140625\n",
      "Batch: 138   Loss= 73.92022705078125\n",
      "Batch: 139   Loss= 35.186458587646484\n",
      "Batch: 140   Loss= 55.836181640625\n",
      "Batch: 141   Loss= 89.99397277832031\n",
      "Batch: 142   Loss= 42.458900451660156\n",
      "Batch: 143   Loss= 94.99451446533203\n",
      "Batch: 144   Loss= 72.01651763916016\n",
      "Batch: 145   Loss= 73.67713928222656\n",
      "Batch: 146   Loss= 98.42085266113281\n",
      "Batch: 147   Loss= 68.98136138916016\n",
      "Batch: 148   Loss= 86.64485931396484\n",
      "Batch: 149   Loss= 78.06880187988281\n",
      "Batch: 150   Loss= 69.58407592773438\n",
      "Batch: 151   Loss= 143.78781127929688\n",
      "Batch: 152   Loss= 97.16715240478516\n",
      "Batch: 153   Loss= 54.61592483520508\n",
      "Batch: 154   Loss= 31.618383407592773\n",
      "Batch: 155   Loss= 92.7098617553711\n",
      "Batch: 156   Loss= 56.08306121826172\n",
      "Batch: 157   Loss= 70.77278137207031\n",
      "Batch: 158   Loss= 72.797607421875\n",
      "Batch: 159   Loss= 48.142234802246094\n",
      "Batch: 160   Loss= 51.427398681640625\n",
      "Batch: 161   Loss= 97.9947280883789\n",
      "Batch: 162   Loss= 61.905033111572266\n",
      "Batch: 163   Loss= 57.77553176879883\n",
      "Batch: 164   Loss= 72.61708068847656\n",
      "Batch: 165   Loss= 76.16767120361328\n",
      "Batch: 166   Loss= 98.34312438964844\n",
      "Batch: 167   Loss= 65.9498519897461\n",
      "Batch: 168   Loss= 75.6348876953125\n",
      "Batch: 169   Loss= 130.72421264648438\n",
      "Batch: 170   Loss= 92.42987823486328\n",
      "Batch: 171   Loss= 50.92448043823242\n",
      "Batch: 172   Loss= 75.40485382080078\n",
      "Batch: 173   Loss= 60.434730529785156\n",
      "Batch: 174   Loss= 95.072998046875\n",
      "Batch: 175   Loss= 77.24812316894531\n",
      "Batch: 176   Loss= 45.886878967285156\n",
      "Batch: 177   Loss= 59.389747619628906\n",
      "Batch: 178   Loss= 93.52682495117188\n",
      "Batch: 179   Loss= 146.52603149414062\n",
      "Batch: 180   Loss= 114.07069396972656\n",
      "Batch: 181   Loss= 113.42625427246094\n",
      "Batch: 182   Loss= 50.58454132080078\n",
      "Batch: 183   Loss= 71.33184051513672\n",
      "Batch: 184   Loss= 73.5615463256836\n",
      "Batch: 185   Loss= 71.04151916503906\n",
      "Batch: 186   Loss= 56.511985778808594\n",
      "Batch: 187   Loss= 61.00390625\n",
      "Batch: 188   Loss= 14.481791496276855\n",
      "Batch: 189   Loss= 70.24408721923828\n",
      "Batch: 190   Loss= 36.9751091003418\n",
      "Batch: 191   Loss= 88.15872192382812\n",
      "Batch: 192   Loss= 32.16597366333008\n",
      "Batch: 193   Loss= 52.141265869140625\n",
      "Batch: 194   Loss= 53.79380798339844\n",
      "Batch: 195   Loss= 72.64993286132812\n",
      "Batch: 196   Loss= 46.82977294921875\n",
      "Batch: 197   Loss= 66.4432373046875\n",
      "Batch: 198   Loss= 58.47874450683594\n",
      "Batch: 199   Loss= 70.5403060913086\n",
      "Batch: 200   Loss= 81.07186126708984\n",
      "✅ Epoch 2 Completed (Trained 200 batches)\n",
      "✅ Training Complete!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW  \n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if torch.cuda.is_available() else None  # ✅ NEW\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "batch_count = 0\n",
    "max_batches_per_epoch = 200 \n",
    "\n",
    "for epoch in range(2):  \n",
    "    batch_count = 0  \n",
    "    for batch in train_loader:\n",
    "        if batch_count >= max_batches_per_epoch:\n",
    "            break  \n",
    "\n",
    "        batch_count += 1  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        if scaler:\n",
    "            with torch.amp.autocast(\"cuda\"):  \n",
    "                outputs = model(input_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Batch: {batch_count}   Loss= {loss.item()}\")  \n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1} Completed (Trained {batch_count} batches)\")\n",
    "\n",
    "print(\"✅ Training Complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "\n",
    "# Save model & processor\n",
    "model.save_pretrained(\"D:/Speech_recognition/wav2vec2_finetuned\")\n",
    "processor.save_pretrained(\"D:/Speech_recognition/wav2vec2_finetuned\")\n",
    "\n",
    "print(\"✅ Model Saved Successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_env",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
